{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd654/babylm-seqlen/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"babylm-seqlen/tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(examples, seq_len):\n",
    "    \"\"\"\n",
    "    Tokenizes and chunks text data to fixed-length sequences.\n",
    "    \n",
    "    Args:\n",
    "        examples: A batch of text examples from the dataset\n",
    "        seq_len: The length of the sequences to chunk the text into\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing chunked token sequences of length SEQ_LEN\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    # Process each text example in the batch\n",
    "    for text in examples['text']:\n",
    "        # Convert text to token IDs\n",
    "        _tokens = tokenizer.encode(text)\n",
    "        # Add EOS token to mark the end of each text example\n",
    "        _tokens.append(tokenizer.eos_token_id)\n",
    "        # Accumulate all tokens in a flat list\n",
    "        tokens.extend(_tokens)\n",
    "\n",
    "    # Split the accumulated tokens into chunks of SEQ_LEN\n",
    "    chunks = [tokens[i:i + seq_len] for i in range(0, len(tokens), seq_len)]\n",
    "    \n",
    "    # Discard the last chunk if it's shorter than SEQ_LEN to ensure uniform sequence length\n",
    "    if len(chunks[-1]) < seq_len:\n",
    "        chunks = chunks[:-1]\n",
    "        \n",
    "    return {'input_ids': chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  simple_wiki.train\n",
      "Processing file:  bnc_spoken.train\n",
      "Processing file:  childes.train\n",
      "Processing file:  open_subtitles.train\n",
      "Processing file:  switchboard.train\n",
      "Processing file:  gutenberg.train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_root_path = 'data/raw/train_100M'\n",
    "\n",
    "raw_data_list= []\n",
    "\n",
    "def raw_data_iterator(data_root_path):\n",
    "    for file in os.listdir(data_root_path):\n",
    "        print(\"Processing file: \", file)\n",
    "        with open(os.path.join(data_root_path, file), 'r') as f:\n",
    "            document = []\n",
    "            for line in f:\n",
    "                if line == '\\n':\n",
    "                    raw_data_list.append({'text': ' '.join(document)})\n",
    "                    document = []\n",
    "                else:\n",
    "                    document.append(line)\n",
    "\n",
    "data_iterator = raw_data_iterator(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_list(raw_data_list)\n",
    "raw_dataset = raw_dataset.shuffle(seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: SINGLE_SHUFFLE means only shuffle the dataset once at the document-level; otherwise, shuffle the dataset a second-time at the tokenized example-level\n",
    "SINGLE_SHUFFLE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 64 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:05<00:00, 1044.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenize_and_chunk_64 = partial(tokenize_and_chunk, seq_len=64)\n",
    "\n",
    "tokenized_dataset_64 = raw_dataset.map(\n",
    "    tokenize_and_chunk_64,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_64 = tokenized_dataset_64.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format:   0%|          | 0/2557 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2557/2557 [00:37<00:00, 69.10ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "664665560"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_64'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_64.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_64.parquet: 100%|██████████| 328M/328M [00:11<00:00, 29.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_64/commit/637997e2668e570527955fd7b747759726239113', commit_message='Upload .parquet with huggingface_hub', commit_description='', oid='637997e2668e570527955fd7b747759726239113', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_64', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_64'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_64\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_64_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_64.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 128 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [01:59<00:00, 1097.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_128 = partial(tokenize_and_chunk, seq_len=128)\n",
    "\n",
    "tokenized_dataset_128 = raw_dataset.map(\n",
    "    tokenize_and_chunk_128,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_128 = tokenized_dataset_128.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format:   0%|          | 0/1279 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1279/1279 [00:21<00:00, 59.62ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "659515080"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_128'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_128.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_128/commit/60a8c27e080391fdbd99e3d7250a1554c4abd938', commit_message='Upload .parquet with huggingface_hub', commit_description='', oid='60a8c27e080391fdbd99e3d7250a1554c4abd938', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_128', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_128'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_128\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_128_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_128.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 256 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:06<00:00, 1037.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_256 = partial(tokenize_and_chunk, seq_len=256)\n",
    "\n",
    "tokenized_dataset_256 = raw_dataset.map(\n",
    "    tokenize_and_chunk_256,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_256 = tokenized_dataset_256.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 640/640 [00:05<00:00, 110.50ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "656894056"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_256'  \n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_256.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_256_single_shuffle.parquet: 100%|██████████| 269M/269M [00:09<00:00, 26.9MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_256_single_shuffle/commit/7d873f0c4c6f48e85dce54e922175b3b3434353f', commit_message='Upload train_100M_256_single_shuffle with huggingface_hub', commit_description='', oid='7d873f0c4c6f48e85dce54e922175b3b3434353f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_256_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_256_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_256\"    \n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_256_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_256.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 512 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:06<00:00, 1033.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_512 = partial(tokenize_and_chunk, seq_len=512)\n",
    "\n",
    "tokenized_dataset_512 = raw_dataset.map(\n",
    "    tokenize_and_chunk_512,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_512 = tokenized_dataset_512.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 320/320 [00:05<00:00, 54.27ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "655480620"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_512'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_512.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_512_single_shuffle.parquet: 100%|██████████| 259M/259M [00:10<00:00, 25.7MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_512_single_shuffle/commit/27964866816815f03d18cde54eb95a14894f88e8', commit_message='Upload train_100M_512.parquet with huggingface_hub', commit_description='', oid='27964866816815f03d18cde54eb95a14894f88e8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_512_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_512_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_512\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_512_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_512.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 1024 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:02<00:00, 1067.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_1024 = partial(tokenize_and_chunk, seq_len=1024)\n",
    "\n",
    "tokenized_dataset_1024 = raw_dataset.map(\n",
    "    tokenize_and_chunk_1024,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_1024 = tokenized_dataset_1024.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 160/160 [00:06<00:00, 26.56ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "654589600"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_1024'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_1024.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_1024_single_shuffle.parquet: 100%|██████████| 252M/252M [00:06<00:00, 40.3MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_1024_single_shuffle/commit/3da02c5ac9104714b1b110614caed758e8ac4431', commit_message='Upload train_100M_1024_single_shuffle with huggingface_hub', commit_description='', oid='3da02c5ac9104714b1b110614caed758e8ac4431', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_1024_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_1024_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_1024\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_1024_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_1024.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 2048 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:02<00:00, 1068.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_2048 = partial(tokenize_and_chunk, seq_len=2048)\n",
    "\n",
    "tokenized_dataset_2048 = raw_dataset.map(\n",
    "    tokenize_and_chunk_2048,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_2048 = tokenized_dataset_2048.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 80/80 [00:06<00:00, 12.33ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "653721156"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_2048'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_2048.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_2048_single_shuffle.parquet: 100%|██████████| 248M/248M [00:09<00:00, 27.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_2048_single_shuffle/commit/33a8cad8d689d508a605a5f2fd798c34b3bac363', commit_message='Upload train_100M_2048_single_shuffle with huggingface_hub', commit_description='', oid='33a8cad8d689d508a605a5f2fd798c34b3bac363', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_2048_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_2048_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_2048\"   \n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_2048_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_2048.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 4096 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:01<00:00, 1079.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_4096 = partial(tokenize_and_chunk, seq_len=4096)\n",
    "\n",
    "tokenized_dataset_4096 = raw_dataset.map(\n",
    "    tokenize_and_chunk_4096,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_4096 = tokenized_dataset_4096.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 40/40 [00:05<00:00,  6.87ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "652471832"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_4096' \n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_4096.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_4096_single_shuffle.parquet: 100%|██████████| 245M/245M [00:07<00:00, 30.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_4096_single_shuffle/commit/9fe08028cf157abfc2f08895ee728af288fa5430', commit_message='Upload train_100M_4096_single_shuffle with huggingface_hub', commit_description='', oid='9fe08028cf157abfc2f08895ee728af288fa5430', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_4096_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_4096_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_4096\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_4096_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_4096.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 8192 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [02:01<00:00, 1079.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_8192 = partial(tokenize_and_chunk, seq_len=8192)\n",
    "\n",
    "tokenized_dataset_8192 = raw_dataset.map(\n",
    "    tokenize_and_chunk_8192,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_8192 = tokenized_dataset_8192.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 20/20 [00:05<00:00,  3.49ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "650327568"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_8192'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_8192.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_8192_single_shuffle.parquet: 100%|██████████| 242M/242M [00:06<00:00, 34.7MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_8192_single_shuffle/commit/a0bba81e3d1159b21000211c0ffc5a566af287f6', commit_message='Upload train_100M_8192_single_shuffle with huggingface_hub', commit_description='', oid='a0bba81e3d1159b21000211c0ffc5a566af287f6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_8192_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_8192_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_8192\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_8192_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_8192.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Len - 16384 Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 131035/131035 [01:59<00:00, 1097.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_and_chunk_16384 = partial(tokenize_and_chunk, seq_len=16384)\n",
    "\n",
    "tokenized_dataset_16384 = raw_dataset.map(\n",
    "    tokenize_and_chunk_16384,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    num_proc=8,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ")\n",
    "if not SINGLE_SHUFFLE:\n",
    "    tokenized_dataset_16384 = tokenized_dataset_16384.shuffle(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:05<00:00,  1.80ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646421020"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_path = 'data/processed/train_100M_16384'\n",
    "if SINGLE_SHUFFLE:\n",
    "    parquet_path += '_single_shuffle'\n",
    "parquet_path += '.parquet'\n",
    "tokenized_dataset_16384.to_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_100M_16384_single_shuffle.parquet: 100%|██████████| 241M/241M [00:06<00:00, 36.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/babylm-seqlen/train_100M_16384_single_shuffle/commit/1ddb02c402ef3abe47c9357089ab42405f5160da', commit_message='Upload train_100M_16384_single_shuffle with huggingface_hub', commit_description='', oid='1ddb02c402ef3abe47c9357089ab42405f5160da', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/babylm-seqlen/train_100M_16384_single_shuffle', endpoint='https://huggingface.co', repo_type='dataset', repo_id='babylm-seqlen/train_100M_16384_single_shuffle'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"babylm-seqlen/train_100M_16384\"\n",
    "if SINGLE_SHUFFLE:\n",
    "    repo_id += \"_single_shuffle\"\n",
    "api.create_repo(repo_id, private=False, exist_ok=True, token=HF_TOKEN, repo_type=\"dataset\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=parquet_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type='dataset',\n",
    "    path_in_repo='train_100M_16384_single_shuffle.parquet' if SINGLE_SHUFFLE else 'train_100M_16384.parquet',\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
